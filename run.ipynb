{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## This notebook is for running `create_tables.py` and `etl.py` and checking if they work properly.\n",
    "- You can create tables for preparing the database and build etl pipeline by runnig the cells below. \n",
    "- This notebook is also used to find in which query has errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE staging_events(\n",
      "    event_id INT IDENTITY(0,1) NOT NULL,\n",
      "    artist_name VARCHAR(300),\n",
      "    auth VARCHAR(50),\n",
      "    user_first_name VARCHAR(300),\n",
      "    user_gender  VARCHAR(1),\n",
      "    item_in_session INTEGER,\n",
      "    user_last_name VARCHAR(300),\n",
      "    song_length DOUBLE PRECISION, \n",
      "    user_level VARCHAR(50),\n",
      "    location VARCHAR(300),\n",
      "    method VARCHAR(30),\n",
      "    page VARCHAR(50),\n",
      "    registration VARCHAR(50),\n",
      "    session_id BIGINT,\n",
      "    song_title VARCHAR(300),\n",
      "    status INTEGER, \n",
      "    ts VARCHAR(50) NOT NULL,\n",
      "    user_agent TEXT,\n",
      "    user_id VARCHAR(100),\n",
      "    PRIMARY KEY (event_id))\n",
      "\n",
      "\n",
      "CREATE TABLE staging_songs(\n",
      "    song_id VARCHAR(100) NOT NULL,\n",
      "    num_songs INTEGER,\n",
      "    artist_id VARCHAR(100) NOT NULL,\n",
      "    artist_latitude DOUBLE PRECISION,\n",
      "    artist_longitude DOUBLE PRECISION,\n",
      "    artist_location VARCHAR(300),\n",
      "    artist_name VARCHAR(300),\n",
      "    title VARCHAR(300),\n",
      "    duration DOUBLE PRECISION,\n",
      "    year INTEGER,\n",
      "    PRIMARY KEY (song_id))\n",
      "\n",
      "\n",
      "CREATE TABLE songplays(\n",
      "    songplay_id INT IDENTITY(0,1),\n",
      "    start_time TIMESTAMP NOT NULL,\n",
      "    user_id VARCHAR(50) NOT NULL,\n",
      "    level VARCHAR(50),\n",
      "    song_id VARCHAR(100) NOT NULL,\n",
      "    artist_id VARCHAR(100) NOT NULL,\n",
      "    session_id BIGINT NOT NULL,\n",
      "    location VARCHAR(300),\n",
      "    user_agent TEXT,\n",
      "    PRIMARY KEY (songplay_id))\n",
      "\n",
      "\n",
      "CREATE TABLE users(\n",
      "    user_id VARCHAR NOT NULL,\n",
      "    first_name VARCHAR(300),\n",
      "    last_name VARCHAR(300),\n",
      "    gender VARCHAR(1),\n",
      "    level VARCHAR(50),\n",
      "    PRIMARY KEY (user_id))\n",
      "\n",
      "\n",
      "CREATE TABLE songs(\n",
      "    song_id VARCHAR(100) NOT NULL,\n",
      "    title VARCHAR(300),\n",
      "    artist_id VARCHAR(100) NOT NULL,\n",
      "    year INTEGER,\n",
      "    duration DOUBLE PRECISION,\n",
      "    PRIMARY KEY (song_id))\n",
      "\n",
      "\n",
      "CREATE TABLE artists(\n",
      "    artist_id VARCHAR(100) NOT NULL,\n",
      "    name VARCHAR(300),\n",
      "    location VARCHAR(300),\n",
      "    latitude DOUBLE PRECISION,\n",
      "    longitude DOUBLE PRECISION,\n",
      "    PRIMARY KEY (artist_id))\n",
      "\n",
      "\n",
      "CREATE TABLE time(\n",
      "    start_time TIMESTAMP NOT NULL,\n",
      "    hour INTEGER,\n",
      "    day INTEGER,\n",
      "    week INTEGER,\n",
      "    month INTEGER,\n",
      "    year INTEGER,\n",
      "    weekday INTEGER,\n",
      "    PRIMARY KEY (start_time))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create_tables.py\n",
    "import configparser\n",
    "import psycopg2\n",
    "from sql_queries import create_table_queries, drop_table_queries\n",
    "\n",
    "\n",
    "def drop_tables(cur, conn):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to drop tables which already exist in the Redshift cluster. This excutes the queries in the variable drop_table_queries in the file sql_queries2.py.\n",
    "    Parameters:\n",
    "    - cur: cursor connection object\n",
    "    - conn: connection object\n",
    "    \"\"\"\n",
    "    \n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        \n",
    "def create_tables(cur, conn):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to create tables in the Redshift cluster. This excutes the queries in the variable create_table_queries in the file sql_queries2.py.\n",
    "    Parameters:\n",
    "    - cur: cursor connection object\n",
    "    - conn: connection object\n",
    "    \"\"\"\n",
    "    \n",
    "    for query in create_table_queries:\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to connect database in the Redshift cluster by reading configurations in the file dwh.cfg. \n",
    "    Additioanlly, this excutes the functions drop_tables(cur, conn) and create_tables(cur, conn).\n",
    "    \"\"\"\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('dwh.cfg')\n",
    "\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    drop_tables(cur, conn)\n",
    "    create_tables(cur, conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy staging_events \n",
      "                          from 's3://udacity-dend/log_data'\n",
      "                          iam_role 'arn:aws:iam::315365539953:role/myRedshiftRole'\n",
      "                          json 's3://udacity-dend/log_json_path.json';\n",
      "                       \n",
      "copy staging_songs \n",
      "                          from 's3://udacity-dend/song_data' \n",
      "                          iam_role 'arn:aws:iam::315365539953:role/myRedshiftRole'\n",
      "                          json 'auto';\n",
      "                      \n",
      "\n",
      "INSERT INTO songplays (start_time, user_id, level, song_id, artist_id, session_id, location, user_agent) \n",
      "SELECT  \n",
      "    DISTINCT TIMESTAMP 'epoch' + se.ts/1000 * interval '1 second' as start_time, \n",
      "    se.user_id, \n",
      "    se.user_level, \n",
      "    ss.song_id,\n",
      "    ss.artist_id, \n",
      "    se.session_id,\n",
      "    se.location, \n",
      "    se.user_agent\n",
      "FROM staging_events se, staging_songs ss\n",
      "WHERE se.page = 'NextSong' \n",
      "AND se.song_title = ss.title \n",
      "AND se.artist_name = ss.artist_name \n",
      "AND se.song_length = ss.duration\n",
      "\n",
      "\n",
      "INSERT INTO users (user_id, first_name, last_name, gender, level)\n",
      "SELECT DISTINCT  \n",
      "    user_id, \n",
      "    user_first_name, \n",
      "    user_last_name, \n",
      "    user_gender, \n",
      "    user_level\n",
      "FROM staging_events\n",
      "WHERE page = 'NextSong' \n",
      "and user_id NOT IN (SELECT DISTINCT user_id FROM users)\n",
      "\n",
      "\n",
      "INSERT INTO songs (song_id, title, artist_id, year, duration) \n",
      "SELECT DISTINCT \n",
      "    song_id, \n",
      "    title,\n",
      "    artist_id,\n",
      "    year,\n",
      "    duration\n",
      "FROM staging_songs\n",
      "WHERE song_id IS NOT NULL\n",
      "and song_id NOT IN (SELECT DISTINCT song_id FROM songs)\n",
      "\n",
      "\n",
      "INSERT INTO artists (artist_id, name, location, latitude, longitude) \n",
      "SELECT DISTINCT \n",
      "    artist_id,\n",
      "    artist_name,\n",
      "    artist_location,\n",
      "    artist_latitude,\n",
      "    artist_longitude\n",
      "FROM staging_songs\n",
      "WHERE artist_id IS NOT NULL\n",
      "and artist_id NOT IN (SELECT DISTINCT artist_id FROM artists)\n",
      "\n",
      "\n",
      "INSERT INTO time(start_time, hour, day, week, month, year, weekDay)\n",
      "SELECT DISTINCT start_time, \n",
      "    extract(hour from start_time),\n",
      "    extract(day from start_time),\n",
      "    extract(week from start_time), \n",
      "    extract(month from start_time),\n",
      "    extract(year from start_time), \n",
      "    extract(dayofweek from start_time)\n",
      "FROM songplays\n",
      "WHERE start_time IS NOT NULL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#etl.py\n",
    "import configparser\n",
    "import psycopg2\n",
    "from sql_queries import copy_table_queries, insert_table_queries\n",
    "\n",
    "\n",
    "def load_staging_tables(cur, conn):\n",
    "     \n",
    "    \"\"\"\n",
    "    Function to extract data from S3 bucket nad copy the contents of data to the stagin tables in the Redshift cluster. \n",
    "    This excutes the queries in the variable copy_table_queries in the file sql_queries2.py.\n",
    "    Parameters:\n",
    "    - cur: cursor connection object\n",
    "    - conn: connection object\n",
    "    \"\"\"\n",
    "    \n",
    "    for query in copy_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def insert_tables(cur, conn):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to transform data from the stagin tables into the fact and dimension tables in the Redshift cluster. \n",
    "    This excutes the queries in the variable insert_table_queries in the file sql_queries2.py.\n",
    "    Parameters:\n",
    "    - cur: cursor connection object\n",
    "    - conn: connection object\n",
    "    \"\"\"\n",
    "    \n",
    "    for query in insert_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to connect database in the Redshift cluster by reading configurations in the file dwh.cfg. \n",
    "    Additioanlly, this excutes the functions load_staging_tables(cur, conn) and insert_tables(cur, conn).\n",
    "    \"\"\"\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('dwh.cfg')\n",
    "\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    load_staging_tables(cur, conn)\n",
    "    insert_tables(cur, conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
